{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "токенизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.tokenize.sent_tokenize(text)\n",
    "nltk.tokenize.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обрабатывать исключения rulebased правила"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нормализация токенов(убрать точки, запятые, пробелы и тд), потом  лемматизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pymorphy2.MorphAnalyzer().normal_forms(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Векторизация текста: bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = sklearn.feature_extraction.text.CountVectorizer()\n",
    "vectorizer.fit([text1, ..., textn])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Векторные представления: Word2Vec, GloVe, FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если учитывать порядок, то нужно N GRAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = sklearn.feature_extraction.text.CountVectorizer(ngram_range=(1, 2))\n",
    "vectorizer.fit([text1, ..., textn])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Методы отбора признаков\n",
    "1. каждому признаку сопостовляется оценка его значимости\n",
    "    \n",
    "    1.1. Статистическое оценивание важности признака (пресижн реколл). false-negative - это вероятность p-value. Чем меньше p, тем более значим признак\n",
    "    \n",
    "    1.2. Логистическая регрессия, регуляризатор L1. установить penalty = 'l1'\n",
    "    \n",
    "    1.3. Случайный лес. oob_score = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Оптимизация гиперпараметров\n",
    "desigion tree: \n",
    "\n",
    "    min_samples_leaf - минимальное число объектов в листе\n",
    "    max_depth - максимальная глубина дерева\n",
    "    \n",
    "random forest:\n",
    "\n",
    "    min_samples_leaf - минимальное число объектов в листе\n",
    "    max_depth - максимальная глубина дерева\n",
    "    n_estimators - кол-во деревьев\n",
    "    \n",
    "logistic regression:\n",
    "\n",
    "    C - константа регуляризации\n",
    "    penalty - вид регуляризатора\n",
    "    max_iter - число итераций\n",
    "    \n",
    "Метод опорных векторов:\n",
    "    \n",
    "    C - константа регуляризации\n",
    "    penalty - вид регуляризатора\n",
    "    kernel - тип поверхности\n",
    "    \n",
    "#### Выбор лучшего гиперпараметра\n",
    "\n",
    "#####    Перебор по сетке - GridSearchCV\n",
    "    \n",
    "    Задаем значения параметров, которые хотим перебрать\n",
    "    \n",
    "    param_grid = {\"param1\": [val1,val2], \"param2\": [val1,val2]}\n",
    "    \n",
    "    Создаем объект gs = GridSearchCV(clf, param_grid)\n",
    "    \n",
    "    Перебираем все наборы гиперпараметров\n",
    "    gs.fit(x,y)\n",
    "    \n",
    "    получаем лучший алгоритм\n",
    "    gs.best_estimator_\n",
    "    \n",
    "    Плюсы: полный перебор, лучшее качество. Минусы: долго\n",
    "    \n",
    "##### RandomSearchCV\n",
    "    \n",
    "    Задаем значения параметров, которые хотим перебрать\n",
    "    \n",
    "    param_grid = {\"param1\": [val1,val2], \"param2\": [val1,val2]}\n",
    "    \n",
    "    Создаем объект gs = RandomSearchCV(clf, param_grid)\n",
    "    \n",
    "    Перебираем случайные наборы гиперпараметров\n",
    "    gs.fit(x,y)\n",
    "    \n",
    "    получаем лучший алгоритм\n",
    "    gs.best_estimator_\n",
    "    \n",
    "    Плюсы: быстрее. Минусы неполный перебор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Объединение алгоритмов\n",
    "\n",
    "    По предсказаниям\n",
    "        \n",
    "        Самое частое предсказание\n",
    "        \n",
    "    По вероятностям\n",
    "        \n",
    "        Среднее значение вероятности\n",
    "        \n",
    "        Взвешенная сумма вероятностей\n",
    "        \n",
    "    Рецепт для смеси двух алгоритмов по вероятности\n",
    "        \n",
    "        Создаем модель, объединяющую две модели ML. Задаем параметр смешивания a. Выдаем предсказание по правилу a*p1+(1-a)*p2. Настраиваем гиперпараметр a. \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Нейронные сети прямого распространения\n",
    "#### Примеры python:\n",
    "\n",
    "from keras.model import Sequential\n",
    "from keras.layers import Dense, activation\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(500, input_shape(100,)))\n",
    "model.add(Activation('relu')\n",
    "\n",
    "два слоя, 500 нейронов, функция активации релу\n",
    "\n",
    "#### Функции активации\n",
    "\n",
    "Функция активации позволяет выдать нелинейное значение с богатым количеством классов. Без нее выходы ничем не отличаются от линейных выходов логистической регрессии\n",
    "\n",
    "Релу легко дифиренцировать, сокращает обучение нейронной сети\n",
    "\n",
    "Гиперболический тангенс замедляет сильно градиентный спуск\n",
    "\n",
    "\n",
    "#### На выходных слоях лучше использовать:\n",
    "    \n",
    "    при бинарной классификации - сигмоиду\\\n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    при многоклассовой - софтмакс\n",
    "    model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "#### Чтобы оценивать нейронную сеть используют функции потерь(loss function) - cтараться минимизировать\n",
    "\n",
    "    бинарная классификация - логистическая функция потерь\n",
    "    \n",
    "    многоклассовая - кросс-энтропия\n",
    "    \n",
    "    Также для классификации можно использовать Hinge-loss or SVM. Неразобрался что где лучше, нужно тестить\n",
    "    \n",
    "    для задач регрессии и auto-encoders использовать MSE(mean square error)\n",
    "    \n",
    "    реализация в керасе: \n",
    "        Model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "        \n",
    "    функция потерь указывается при компиляции нейронной сети, перед запуском обучения\n",
    "    \n",
    "    Обучение происходит с помощью градиентного спуска. Лучше использовать оптимизации стохастического град.спуска, такие как NAG, Adagrad. Но быстрее всего сходятся adadelta, rmsprop. Крч не юзать простой град. спуск и стохаст. град. спуск.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "#### Виды распространения ошибки\n",
    "    \n",
    "    Прямое распространение. Находим функцию потерь.\n",
    "    \n",
    "    Обратное распространение(back propogation). Находим необходимые градиенты и осуществляем итерацию градиентного спуска."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Слои, нейроны и их количество\n",
    "    Правило:\n",
    "    \n",
    "    Начинаем с простейших малых архитектур нейронной сети с малым количеством нейронов и одним скрытым слоем.\n",
    "    \n",
    "        Увеличивать количество нейронов в слое, пока уменьшается функция потерь.\n",
    "        \n",
    "        После, когда градиентный спуск больше не уменьшает функцию потерь при увеличении нейронов в слое, то добавляется НОВЫЙ СЛОЙ.\n",
    "        \n",
    "        Так получится логичная простая архитектура для конкретной задачи\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Недообученность и переобученность сети\n",
    "    \n",
    "    После проделанных действий сеть может быть недообучена или переобучена.\n",
    "    \n",
    "    Причины:\n",
    "        недообученность:\n",
    "        \n",
    "        Первое: представление данных. Шум, разный формат, не соответствовать ожиданиям. Т.е. первым делом проверить данные.\n",
    "        \n",
    "        Второе: несбалансированность данных. Решение: undersampling, oversampling\n",
    "        \n",
    "        Переобученность:\n",
    "        \n",
    "        Несовпадение распределений в тренировочном и тестовом множестве. Т.е. например плохие предсказания на определенном классе. В тренировочных данных данные определенного класса очень в малом количестве, а в тестовом их много. Нужно равномерное распределение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Методы борьбы с недообучением и переобучением в Keras\n",
    "\n",
    "Первая проблема: Проблемы недообучения сети может быть связана с тем, что текущее значение весов W соответствует локальному минимуму функции потерь. А если мы находимся в локальном минимуме, то это значит, что итерация градиентного спуска не изменяет вектора весов, т.к. delta L(W) = 0.\n",
    "    \n",
    "    Решение: варьирование инициализации весов. Источником проблемы может быть нулевая инициализация, в результате которой веса нейронов в одном нейронном слое инициализируются одинаково и потом изменяются по одним и тем же правилам. В результате все нейроны слоя могут иметь одинаковые веса. В итоге это по сути сложная сеть с одним нейроном, т.к. они все одинаковые. \n",
    "    \n",
    "    Так что лучше всего инициализация случайными небольшими значениями.\n",
    "    \n",
    "    Более продвинутые методы: инициализация хавьера и Хе инициализация\n",
    "    \n",
    "    Еще можно мультистарт\n",
    "    \n",
    "Вторая проблема: проблема градиентного спуска: \n",
    "\n",
    "Взрыв градиентного спуска(слишком большие значения)\n",
    "\n",
    "Затухание градиентного спуска(слишком маленькие значения)\n",
    "\n",
    "    Второе решение: оптимизаторы градиентного спуска. С помощью них можно уйти из локальных минимумов. Я писал их выше в функциях потерь.\n",
    "    NAG, Adagrad, Adam. Но быстрее всего сходятся adadelta, rmsprop\n",
    "    \n",
    "    реализация в керасе: \n",
    "        Model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "        \n",
    "        Указывается перед компиляцией.\n",
    "        \n",
    "Третья проблема: \n",
    "    \n",
    "    Значения признаков могут быть несбалансированы, что приведет к нестабильной работе град. спуска. Т.к. от батча к батчу могут возникать значения разных порядков. В НЛП такое не происходит на входном слое, но может быть на скрытых и выходном.\n",
    "    \n",
    "    Решение: нормализация выходов всех нейронов перед их использованием в следующем слое. Т.е. после каждого слоя выходы нейронов образуют некоторую выборку чисел. Вычитаем из выборки среднее и делим на корень дисперсии. После используем некоторое линейное преобразование ay+b, где a и b параметры слоя, которые настраиваются в ходе град. спуска.\n",
    "    В итоге будет более сбалансированный вектор выходных значений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Переобучение (точность на обучении выше, чем на тесте)\n",
    "    \n",
    "Проблема: Переобучение связано с большим количеством внутренних параметров\n",
    "\n",
    "Решение: Добавление регуляризатора. К функции потерь добавляется некоторая функция потерь, которая штрафует нас за большие значения коэффициентов, большое количество связей.\n",
    "    \n",
    "    Стандартные регуляризаторы: L1, L2\n",
    "\n",
    "    Пример питон:\n",
    "    model.add(Dense)n_nodes,\n",
    "        kernel_regularizer = regularizers.l1(C),\n",
    "        bias_regularizer = regularizers.l1(C)))\n",
    "        \n",
    "    Регуляризация подобно инициализации весов является параметром слоя нейронной сети\n",
    "    \n",
    "    \n",
    "Второе решение: введение dropout. Задаем число p (0,1) - вероятность. Затем проходим по всем нейронам нейронной сети и отключаем нейрон с вероятостью p. Отключение - это обнуление коэффициентов нейрона, связывающих с предыдущим и след. слоем.\n",
    "\n",
    "Это будто регуляризатор, который позволит предотвратить лишние связи.\n",
    "\n",
    "Параметр p позволяет найти золотую середину между уменьшением функции потерь и предотвращением излишних внутренних связей.\n",
    "\n",
    "Dropout используется только на этапе обучения модели. При тестировании используются все нейроны.\n",
    "\n",
    "В керасе дропаут нужно добавлять отдельным слоем после каждого слоя нейронной сети, в которой мы хотим выключить часть нейронов.\n",
    "\n",
    "Пример: model.add(Dropout(0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Существуют сети прямого распространения NN, RNN - рекурентные нейронные сети, LSTM - сети долгой краткосрочной памяти, CNN - конвулионные нейронные сети.\n",
    "\n",
    "RNN и CNN используются, когда данные не структурированы. Например кол-во входов неравно кол-ву выходов.\n",
    "\n",
    "Примеры: генерация речи, музыки, машинный перевод, выделение именованных сущностей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выделение сущностей\n",
    "Выделение сущностей в тексте - это NER: имена, названия, дата, время, номер и тд\n",
    "\n",
    "Sequеnce tagging - сопостовлени словам различных тегов: часть речи, форма слова\n",
    "\n",
    "Slot filling - заполнение ячеек по тексту: время отправления, дата прибытия\n",
    "\n",
    "### rule-based \n",
    "\n",
    "    Наивные подходы:\n",
    "\n",
    "        Регулярные выражения\n",
    "\n",
    "        Использование словарей\n",
    "\n",
    "    ML к задаче NER:\n",
    "        \n",
    "        Стандартные модели\n",
    "        \n",
    "        RNN, LSTM\n",
    "        \n",
    "        CRF\n",
    "\n",
    "\n",
    "https://www.coursera.org/lecture/machinnoe-obuchenie-v-finansah/zadachi-ner-eDPvK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
